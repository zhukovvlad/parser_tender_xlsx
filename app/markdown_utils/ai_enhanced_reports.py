"""
–ú–æ–¥—É–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ MD –æ—Ç—á–µ—Ç–æ–≤ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π AI –¥–∞–Ω–Ω—ã—Ö.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–∑–¥–∞–µ—Ç –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–µ markdown –æ—Ç—á–µ—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞—Ç:
1. –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Excel —Ñ–∞–π–ª–∞ (–∏—Å–ø–æ–ª—å–∑—É—è json_to_markdown.py)
2. AI –¥–∞–Ω–Ω—ã–µ, –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–æ—Ç–∞, –Ω–æ –ø–µ—Ä–µ–¥ —Ä–∞—Å—á–µ—Ç–Ω–æ–π —Å—Ç–æ–∏–º–æ—Å—Ç—å—é
"""

import json
import logging
from pathlib import Path
from typing import Any, Dict, List

from .json_to_markdown import generate_markdown_for_lots

log = logging.getLogger(__name__)


def regenerate_reports_with_ai_data(
    tender_data: Dict[str, Any], ai_results: List[Dict], db_id: str, lot_ids_map: Dict[str, int]
) -> bool:
    """
    –°–æ–∑–¥–∞–µ—Ç MD –æ—Ç—á–µ—Ç—ã —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π AI –¥–∞–Ω–Ω—ã—Ö.

    Args:
        tender_data: –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–µ–Ω–¥–µ—Ä–∞ –∏–∑ Excel
        ai_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã AI –æ–±—Ä–∞–±–æ—Ç–∫–∏
        db_id: ID —Ç–µ–Ω–¥–µ—Ä–∞ –≤ –ë–î
        lot_ids_map: –ú–∞–ø–ø–∏–Ω–≥ –ª–æ—Ç–æ–≤ –∫ –∏—Ö ID –≤ –ë–î

    Returns:
        True –µ—Å–ª–∏ –æ—Ç—á–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã
    """
    log.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ MD –æ—Ç—á–µ—Ç–æ–≤ —Å AI –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {db_id}")

    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º MD –æ—Ç—á–µ—Ç—ã —Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ AI –¥–∞–Ω–Ω—ã–º–∏
        lot_markdowns, initial_metadata = generate_markdown_for_lots(
            data=tender_data, ai_results=ai_results, lot_ids_map=lot_ids_map
        )

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ª–æ—Ç
        success_count = 0
        for lot_key, markdown_lines in lot_markdowns.items():
            real_lot_id = lot_ids_map.get(lot_key)
            if not real_lot_id:
                log.warning(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω —Ä–µ–∞–ª—å–Ω—ã–π ID –¥–ª—è –ª–æ—Ç–∞ {lot_key}")
                continue

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π MD —Ñ–∞–π–ª
            if _save_enriched_markdown(markdown_lines, db_id, real_lot_id):
                success_count += 1

                # –°–æ–∑–¥–∞–µ–º chunks —Ñ–∞–π–ª
                _create_chunks_file(markdown_lines, db_id, real_lot_id, initial_metadata, lot_key)

        log.info(f"‚úÖ MD –æ—Ç—á–µ—Ç—ã —Å AI –¥–∞–Ω–Ω—ã–º–∏ —Å–æ–∑–¥–∞–Ω—ã –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ {db_id}: {success_count} —Ñ–∞–π–ª–æ–≤")
        return success_count > 0

    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ MD –æ—Ç—á–µ—Ç–æ–≤ —Å AI –¥–∞–Ω–Ω—ã–º–∏: {e}")
        return False


def _save_enriched_markdown(markdown_lines: List[str], tender_id: str, lot_id: int) -> bool:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π markdown —Ñ–∞–π–ª.

    Returns:
        True –µ—Å–ª–∏ —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω
    """
    try:
        output_dir = Path("tenders_md")
        output_dir.mkdir(exist_ok=True)

        filename = f"{tender_id}_{lot_id}.md"
        filepath = output_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            f.write("\n".join(markdown_lines))

        log.info(f"üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π MD —Ñ–∞–π–ª: {filepath}")
        return True

    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è MD —Ñ–∞–π–ª–∞ –¥–ª—è –ª–æ—Ç–∞ {lot_id}: {e}")
        return False


def _create_chunks_file(
    markdown_lines: List[str], tender_id: str, lot_id: int, initial_metadata: Dict[str, Any], lot_key: str
):
    """
    –°–æ–∑–¥–∞–µ—Ç chunks —Ñ–∞–π–ª –∏–∑ –æ–±–æ–≥–∞—â–µ–Ω–Ω–æ–≥–æ markdown.
    –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π langchain –¥–ª—è —Ä–∞–±–æ—Ç—ã.
    """
    try:
        # –õ–µ–Ω–∏–≤—ã–π –∏–º–ø–æ—Ä—Ç - —Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ —Ä–µ–∞–ª—å–Ω–æ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å chunks
        from ..markdown_to_chunks.tender_chunker import create_chunks_from_markdown_text
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º markdown –≤ –æ–¥–∏–Ω —Ç–µ–∫—Å—Ç
        markdown_text = "\n".join(markdown_lines)

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è chunks
        tender_metadata = {
            "tender_id": str(tender_id),
            "lot_id": lot_id,
            "tender_title": initial_metadata.get("tender_title", f"—Ç–µ–Ω–¥–µ—Ä {tender_id}"),
            "executor_name": initial_metadata.get("executor_name", "–Ω–µ —É–∫–∞–∑–∞–Ω"),
            "lot_title": f"{lot_key}: –¥–∞–Ω–Ω—ã–µ –ª–æ—Ç–∞",
        }

        # –°–æ–∑–¥–∞–µ–º chunks
        chunks = create_chunks_from_markdown_text(markdown_text, tender_metadata, lot_id)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º chunks —Ñ–∞–π–ª
        output_dir = Path("tenders_chunks")
        output_dir.mkdir(exist_ok=True)

        filename = f"{tender_id}_{lot_id}_chunks.json"
        filepath = output_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(chunks, f, ensure_ascii=False, indent=2)

        log.info(f"üì¶ –°–æ–∑–¥–∞–Ω chunks —Ñ–∞–π–ª: {filepath}")

    except ImportError as e:
        log.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å–æ–∑–¥–∞–Ω–∏—è chunks –¥–ª—è –ª–æ—Ç–∞ {lot_id}: langchain –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ({e})")
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è chunks —Ñ–∞–π–ª–∞ –¥–ª—è –ª–æ—Ç–∞ {lot_id}: {e}")
