# app/parse_with_gemini.py

"""
–ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è parse.py —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π Gemini AI –æ–±—Ä–∞–±–æ—Ç–∫–∏.

–≠—Ç–æ—Ç —Ñ–∞–π–ª —Ä–∞—Å—à–∏—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª parse.py, –¥–æ–±–∞–≤–ª—è—è:
1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å GeminiWorker / TenderProcessor –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞ –ª–æ—Ç–æ–≤
2. –ü–æ–¥–¥–µ—Ä–∂–∫—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ Redis (—á–µ—Ä–µ–∑ –≤–∞—à workers.gemini)
3. –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
4. –£–ª—É—á—à–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫

–ü—Ä–∏–º–µ—á–∞–Ω–∏—è –ø–æ —Å—Ç–∏–ª—é / –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è:
- –ù–ï –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º logging.basicConfig –Ω–∞ —É—Ä–æ–≤–Ω–µ –º–æ–¥—É–ª—è (—á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å –∫–æ–Ω—Ñ–∏–≥ —Å–µ—Ä–≤–∏—Å–∞/–≤–æ—Ä–∫–µ—Ä–∞).
- –ù–ï –≤—ã–∑—ã–≤–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—ã–π load_dotenv –≤–Ω—É—Ç—Ä–∏ —Ñ—É–Ω–∫—Ü–∏–π.
- –ß–∏–Ω–∏–º mkdir –¥–ª—è –ø—É—Ç–µ–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.
- –ü–æ–¥–ø—Ä–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç GeminiIntegration (—Ç–æ—á–Ω—ã–π –ø—É—Ç—å –≤ –ø–∞–∫–µ—Ç–µ app.workers.gemini).
- –í CLI –æ—Å—Ç–∞–≤–ª–µ–Ω—ã –æ–±–∞ —Ñ–ª–∞–≥–∞: --async –∏ --async-mode (–æ–±–∞ –º–∞–ø—è—Ç—Å—è –≤ async_mode) –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests

from app.gemini_module.logger import get_gemini_logger
from app.workers.gemini.integration import GeminiIntegration

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞ –æ–¥–∏–Ω —Ä–∞–∑ ‚Äî –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ –º–æ–¥—É–ª—è
try:
    from dotenv import load_dotenv

    load_dotenv()
except ImportError:
    pass  # dotenv –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

# –õ–æ–≥–≥–µ—Ä –º–æ–¥—É–ª—è (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è)
log = logging.getLogger(__name__)


# –ò–º–ø–æ—Ä—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ Gemini (–≤–æ—Ä–∫–µ—Ä–Ω–∞—è –æ–±—ë—Ä—Ç–∫–∞) –∏ –ª–æ–≥–≥–µ—Ä–∞ –º–æ–¥—É–ª—è

# –ò–º–ø–æ—Ä—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
# (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ –≤—ã–∑—ã–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π parse_file, –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ —Ç—É—Ç)
# from .parse import parse_file as original_parse_file  # <- –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è


def parse_file_with_gemini(
    xlsx_path: str,
    enable_ai: bool = False,
    async_processing: bool = False,
    redis_config: Optional[Dict] = None,
) -> bool:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É XLSX, –∑–∞—Ç–µ–º –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –∑–∞–ø—É—Å–∫–∞–µ—Ç AI –æ–±—Ä–∞–±–æ—Ç–∫—É (Gemini).

    Args:
        xlsx_path: –ü—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É
        enable_ai: –í–∫–ª—é—á–∏—Ç—å AI –æ–±—Ä–∞–±–æ—Ç–∫—É (—Ç—Ä–µ–±—É–µ—Ç GOOGLE_API_KEY)
        async_processing: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —á–µ—Ä–µ–∑ Redis (—Å–º. workers.gemini)
        redis_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Redis –¥–ª—è async —Ä–µ–∂–∏–º–∞

    Returns:
        True –µ—Å–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ (–¥–∞–∂–µ –±–µ–∑ AI), False –ø—Ä–∏ —Ñ–∞—Ç–∞–ª—å–Ω–æ–π –æ—à–∏–±–∫–µ –±–∞–∑–æ–≤–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞
    """
    gemini_logger = get_gemini_logger()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å AI –æ–±—Ä–∞–±–æ—Ç–∫–∏
    gemini_enabled = bool(os.getenv("GOOGLE_API_KEY"))
    ai_will_be_used = bool(enable_ai and gemini_enabled)

    if enable_ai and not gemini_enabled:
        gemini_logger.warning("‚ö†Ô∏è AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞, –Ω–æ GOOGLE_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ AI.")
    elif not enable_ai:
        log.info("AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–∞ (enable_ai=False)")
    else:
        log.info("AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∫–ª—é—á–µ–Ω–∞: GOOGLE_API_KEY –Ω–∞–π–¥–µ–Ω, enable_ai=True")

    # –í—ã–ø–æ–ª–Ω—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º ID
    log.info("üîÑ –í—ã–ø–æ–ª–Ω—è—é —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞‚Ä¶")

    try:
        # –í–°–ï–ì–î–ê —Å–æ–∑–¥–∞–µ–º positions —Ñ–∞–π–ª—ã - –æ–Ω–∏ –Ω—É–∂–Ω—ã –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏
        # –ü–µ—Ä–µ–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, –±—É–¥–µ—Ç –ª–∏ AI –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è
        db_id, lot_ids_map, tender_data = parse_with_ids(
            xlsx_path, 
            create_reports=True,
            will_use_ai=ai_will_be_used
        )

        if not db_id:
            log.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å ID –æ—Ç Go-—Å–µ—Ä–≤–µ—Ä–∞")
            return False

        log.info("‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. Tender DB ID: %s", db_id)
        log.debug("üìã –ü–æ–ª—É—á–µ–Ω—ã ID –ª–æ—Ç–æ–≤: %s", lot_ids_map)
    except Exception:
        log.exception("‚ùå –û—à–∏–±–∫–∞ –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ")
        return False

    # –ï—Å–ª–∏ AI –Ω–µ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è, –∑–∞–≤–µ—Ä—à–∞–µ–º
    if not ai_will_be_used:
        log.info("‚ÑπÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –±–µ–∑ AI –∞–Ω–∞–ª–∏–∑–∞")
        return True

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–π ID (fallback —Ä–µ–∂–∏–º)
    if str(db_id).startswith("temp_"):
        gemini_logger.warning("‚ö†Ô∏è –ü–æ–ª—É—á–µ–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π ID ‚Äî AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        gemini_logger.info("‚ÑπÔ∏è AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–æ–≤ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ ID")
        gemini_logger.info("‚ÑπÔ∏è –î–æ—Å—Ç—É–ø–Ω—ã –±–∞–∑–æ–≤—ã–µ _positions —Ñ–∞–π–ª—ã")
        return True

    # –ó–∞–ø—É—Å–∫–∞–µ–º AI –æ–±—Ä–∞–±–æ—Ç–∫—É —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ ID
    return process_tender_with_gemini_ids(db_id, lot_ids_map, tender_data, async_processing, redis_config)


def parse_with_ids(
    xlsx_path: str, 
    create_reports: bool = True,
    will_use_ai: bool = False
) -> tuple[Optional[str], Optional[Dict[str, int]], Optional[Dict]]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ ID –∏ –¥–∞–Ω–Ω—ã–µ.

    Args:
        xlsx_path: –ü—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É
        create_reports: –°–æ–∑–¥–∞–≤–∞—Ç—å –ª–∏ positions —Ñ–∞–π–ª—ã (–æ–±—ã—á–Ω–æ True ‚Äî –Ω—É–∂–Ω—ã –¥–ª—è AI)
        will_use_ai: –ë—É–¥–µ—Ç –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–≤–ª–∏—è–µ—Ç –Ω–∞ —Å–æ–∑–¥–∞–Ω–∏–µ MD/chunks)

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (db_id, lot_ids_map, tender_data) –∏–ª–∏ (None, None, None) –ø—Ä–∏ –æ—à–∏–±–∫–µ
    """
    import openpyxl
    from openpyxl.worksheet.worksheet import Worksheet

    from .excel_parser.postprocess import (
        normalize_lots_json_structure,
        replace_div0_with_null,
    )
    from .excel_parser.read_executer_block import read_executer_block
    from .excel_parser.read_headers import read_headers
    from .excel_parser.read_lots_and_boundaries import read_lots_and_boundaries

    source_path = Path(xlsx_path)
    if not source_path.exists():
        log.error("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", xlsx_path)
        return None, None, None

    # –≠—Ç–∞–ø 1: –ü–∞—Ä—Å–∏–Ω–≥ XLSX
    log.info("üîÑ –ü–∞—Ä—Å–∏–Ω–≥ XLSX —Ñ–∞–π–ª–∞‚Ä¶")
    wb = None
    try:
        wb = openpyxl.load_workbook(source_path, data_only=True)
        ws: Worksheet = wb.active

        processed_data: Dict[str, Any] = {
            **read_headers(ws),
            "executor": read_executer_block(ws),
            "lots": read_lots_and_boundaries(ws),
        }
        processed_data = normalize_lots_json_structure(processed_data)
        processed_data = replace_div0_with_null(processed_data)
        log.info("‚úÖ XLSX —Ñ–∞–π–ª —É—Å–ø–µ—à–Ω–æ —Ä–∞–∑–æ–±—Ä–∞–Ω")
    except Exception:
        log.exception("‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XLSX")
        return None, None, None
    finally:
        try:
            if wb is not None:
                wb.close()
        except Exception:
            pass

    # –≠—Ç–∞–ø 2: –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –Ω–∞ Go-—Å–µ—Ä–≤–µ—Ä–µ
    log.info("üîÑ –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ç–µ–Ω–¥–µ—Ä–∞ –Ω–∞ Go-—Å–µ—Ä–≤–µ—Ä–µ‚Ä¶")

    try:
        db_id, lot_ids_map = _import_full_tender_via_go(processed_data)
    except Exception as e:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ —Ç–µ–Ω–¥–µ—Ä–∞ –Ω–∞ Go-—Å–µ—Ä–≤–µ—Ä–µ: {e}")
        return None, None, None

    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π JSON –ª–æ–∫–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –≤–∫–ª—é—á—ë–Ω SAVE_DEBUG_FILES
    if os.getenv("SAVE_DEBUG_FILES", "false").lower() == "true":
        try:
            out_dir = Path("tenders_json")
            out_dir.mkdir(parents=True, exist_ok=True)
            base_json_path = out_dir / f"{db_id}_base.json"
            with open(base_json_path, "w", encoding="utf-8") as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            log.info("üíæ –ë–∞–∑–æ–≤—ã–π JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: %s", base_json_path)
        except Exception:
            log.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –±–∞–∑–æ–≤—ã–π JSON", exc_info=True)

    # –≠—Ç–∞–ø 3: –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ ID
    # –°–æ–≥–ª–∞—Å–Ω–æ –¥–∏–∞–≥—Ä–∞–º–º–µ –ø–∞–π–ø–ª–∞–π–Ω–∞:  # noqa: RUF001
    # 1. Positions —Ñ–∞–π–ª—ã (–¥–ª—è AI)
    # 2. –ü–æ–ª–Ω—ã–π MD —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ç–µ–Ω–¥–µ—Ä–∞ (json_to_markdown)
    # 3. –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π MD —Å –∫–ª—é—á–µ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ + AI –¥–∞–Ω–Ω—ã–º–∏
    # 4. Chunks –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î
    if create_reports:
        log.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤‚Ä¶")  # noqa: RUF001
        try:
            from .markdown_utils.positions_report import generate_reports_for_all_lots
            from .markdown_utils.json_to_markdown import generate_markdown_for_lots

            # 3.1 –í–°–ï–ì–î–ê —Å–æ–∑–¥–∞–µ–º positions —Ñ–∞–π–ª—ã (–Ω—É–∂–Ω—ã –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏)
            output_dir = Path("tenders_positions")
            output_dir.mkdir(parents=True, exist_ok=True)
            base_name = db_id  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π DB ID

            _ = generate_reports_for_all_lots(processed_data, output_dir, base_name, lot_ids_map)
            log.info("‚úÖ Positions —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–Ω—ã —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ ID")  # noqa: RUF001

            # 3.2 –í–°–ï–ì–î–ê —Å–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π MD —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ç–µ–Ω–¥–µ—Ä–∞ (–ë–ï–ó AI –¥–∞–Ω–Ω—ã—Ö)
            # –≠—Ç–æ –±–∞–∑–æ–≤—ã–π MD –∏–∑ JSON - —à–∞–≥ 2 –≤ –¥–∏–∞–≥—Ä–∞–º–º–µ
            log.info("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ MD —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ç–µ–Ω–¥–µ—Ä–∞ (–∏–∑ JSON)...")  # noqa: RUF001
            lot_markdowns, _initial_metadata = generate_markdown_for_lots(processed_data)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π –ø–æ–ª–Ω—ã–π MD –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ª–æ—Ç–∞
            base_md_dir = Path("tenders_md_base")
            base_md_dir.mkdir(parents=True, exist_ok=True)
            
            for lot_key, markdown_lines in lot_markdowns.items():
                real_lot_id = lot_ids_map.get(lot_key)
                if real_lot_id:
                    base_md_path = base_md_dir / f"{db_id}_{real_lot_id}_base.md"
                    with open(base_md_path, "w", encoding="utf-8") as f:
                        f.write("\n".join(markdown_lines))
                    log.info(f"üìÑ –°–æ—Ö—Ä–∞–Ω–µ–Ω –±–∞–∑–æ–≤—ã–π MD: {base_md_path.name}")  # noqa: RUF001
            
            log.info("‚úÖ –ü–æ–ª–Ω—ã–π MD —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º —Ç–µ–Ω–¥–µ—Ä–∞ —Å–æ–∑–¥–∞–Ω")  # noqa: RUF001

            # 3.3 –°–æ–∑–¥–∞–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π MD –∏ chunks
            # –ï—Å–ª–∏ AI –ù–ï –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è - —Å–æ–∑–¥–∞–µ–º —Å—Ä–∞–∑—É —Å –∑–∞–≥–ª—É—à–∫–æ–π
            # –ï—Å–ª–∏ AI –±—É–¥–µ—Ç - —Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–ª–æ–∂–∏—Ç—Å—è –¥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö AI –¥–∞–Ω–Ω—ã—Ö
            if not will_use_ai:
                log.info("üîÑ AI –Ω–µ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è - —Å–æ–∑–¥–∞–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π MD —Å –∑–∞–≥–ª—É—à–∫–æ–π")  # noqa: RUF001
                from .markdown_utils.ai_enhanced_reports import regenerate_reports_with_ai_data
                
                # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É –¥–ª—è AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                ai_stub_results = []
                for _lot_key, lot_db_id in lot_ids_map.items():
                    ai_stub_results.append({
                        "lot_id": lot_db_id,
                        "category": "Test mode",
                        "ai_data": {"message": "No data. Test mode"},
                        "processed_at": "",
                        "status": "stub"
                    })
                
                # –°–æ–∑–¥–∞–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π MD —Å –∑–∞–≥–ª—É—à–∫–æ–π
                success = regenerate_reports_with_ai_data(
                    tender_data=processed_data,
                    ai_results=ai_stub_results,
                    db_id=str(db_id),
                    lot_ids_map=lot_ids_map
                )
                
                if success:
                    log.info("‚úÖ –û–±–æ–≥–∞—â–µ–Ω–Ω—ã–π MD –∏ chunks —Å–æ–∑–¥–∞–Ω—ã —Å –∑–∞–≥–ª—É—à–∫–æ–π AI –¥–∞–Ω–Ω—ã—Ö")
                else:
                    log.warning("‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ–±–æ–≥–∞—â–µ–Ω–Ω–æ–≥–æ MD —Å –∑–∞–≥–ª—É—à–∫–æ–π")
            else:
                log.info("‚ÑπÔ∏è AI –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è - –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π MD –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                
        except Exception:
            log.exception("‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ)")
    else:
        log.info("‚ÑπÔ∏è –ü—Ä–æ–ø—É—Å–∫–∞—é —Å–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤")

    return db_id, lot_ids_map, processed_data


def process_tender_with_gemini_ids(
    tender_db_id: str,
    lot_ids_map: Dict[str, int],
    tender_data: Dict,
    async_processing: bool = False,
    redis_config: Optional[Dict] = None,
) -> bool:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç AI –æ–±—Ä–∞–±–æ—Ç–∫—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω—ã—Ö ID –∏–∑ –ë–î.

    –ü—Ä–∏ async_processing=True –∑–∞–ø—É—Å–∫–∞–µ—Ç Celery –∑–∞–¥–∞—á–∏ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞).
    –ü—Ä–∏ async_processing=False –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–∞–ø—Ä—è–º—É—é.
    """
    gemini_logger = get_gemini_logger()
    gemini_logger.info("üß† –ù–∞—á–∏–Ω–∞—é AI –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–Ω–¥–µ—Ä–∞ %s (async=%s)", tender_db_id, async_processing)

    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        gemini_logger.warning("‚ö†Ô∏è GOOGLE_API_KEY –Ω–µ –∑–∞–¥–∞–Ω ‚Äî AI –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        return True  # –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫–∞–∫ ¬´–±–µ–∑ AI¬ª

    positions_dir = Path("tenders_positions")

    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Celery
    if async_processing:
        gemini_logger.info("üîÑ –†–µ–∂–∏–º: –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Celery")
        try:
            celery_tasks_queued = 0
            gemini_logger.info("üîç –ò—â—É —Ñ–∞–π–ª—ã –ø–æ–∑–∏—Ü–∏–π –≤ %s –¥–ª—è –ª–æ—Ç–æ–≤: %s", positions_dir, lot_ids_map)

            for lot_key, lot_db_id in lot_ids_map.items():
                positions_file = positions_dir / f"{tender_db_id}_{lot_db_id}_positions.md"

                if positions_file.exists():
                    gemini_logger.info("üîÑ –ó–∞–ø—É—Å–∫–∞—é Celery –∑–∞–¥–∞—á—É –¥–ª—è –ª–æ—Ç–∞ %s (—Ñ–∞–π–ª: %s)", lot_db_id, positions_file.name)

                    # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º Celery –∑–∞–¥–∞—á—É
                    from app.workers.gemini.tasks import process_tender_positions

                    task = process_tender_positions.delay(
                        tender_id=str(tender_db_id),
                        lot_id=str(lot_db_id),
                        positions_file_path=str(positions_file),
                        api_key=api_key,
                    )
                    gemini_logger.info("‚úÖ Celery –∑–∞–¥–∞—á–∞ –∑–∞–ø—É—â–µ–Ω–∞: %s –¥–ª—è –ª–æ—Ç–∞ %s", task.id, lot_db_id)
                    celery_tasks_queued += 1
                else:
                    gemini_logger.warning("‚ö†Ô∏è –§–∞–π–ª –ø–æ–∑–∏—Ü–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –ª–æ—Ç–∞ %s: %s", lot_db_id, positions_file)

            if celery_tasks_queued > 0:
                gemini_logger.info("üöÄ –ó–∞–ø—É—â–µ–Ω–æ %d Celery –∑–∞–¥–∞—á –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–æ—Ç–æ–≤", celery_tasks_queued)
                gemini_logger.info("‚ÑπÔ∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ Go —Å–µ—Ä–≤–µ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∑–∞–¥–∞—á")
                return True
            else:
                gemini_logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è AI –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return False

        except Exception as e:
            gemini_logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ Celery –∑–∞–¥–∞—á: %s", e)
            gemini_logger.info("üîÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Ä–µ–∑–µ—Ä–≤–Ω–æ–º—É —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º—É —Ä–µ–∂–∏–º—É")
            # Fallthrough –∫ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ

    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
    gemini_logger.info("üîÑ –†–µ–∂–∏–º: —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
    try:
        integration = GeminiIntegration(api_key=api_key)
        lots_data = integration.create_positions_file_data(tender_db_id, tender_data, lot_ids_map)

        if not lots_data:
            gemini_logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return False

        gemini_logger.info("üìã –ó–∞–ø—É—Å–∫–∞—é —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é AI –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è %d –ª–æ—Ç–æ–≤", len(lots_data))
        results = integration.process_tender_lots_sync(tender_db_id, lots_data)

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ë–î
        successful_sends = 0
        for result in results:
            if result.get("status") == "success":
                from app.json_to_server.ai_results_client import (
                    save_ai_results_offline,
                    send_lot_ai_results,
                )

                ok, status_code, _ = send_lot_ai_results(
                    tender_id=result.get("tender_id"),
                    lot_id=result.get("lot_id"),
                    category=result.get("category", ""),
                    ai_data=result.get("ai_data", {}),
                    processed_at=result.get("processed_at", ""),
                )

                if ok:
                    gemini_logger.info(
                        "üíæ AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ Go –¥–ª—è %s_%s (status=%s)",
                        result.get("tender_id"),
                        result.get("lot_id"),
                        status_code,
                    )
                    successful_sends += 1
                else:
                    offline_path = save_ai_results_offline(
                        tender_id=result.get("tender_id"),
                        lot_id=result.get("lot_id"),
                        category=result.get("category", ""),
                        ai_data=result.get("ai_data", {}),
                        processed_at=result.get("processed_at", ""),
                        reason="request_failed",
                    )
                    gemini_logger.warning("üì¶ Go –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –æ—Ñ—Ñ–ª–∞–π–Ω: %s", offline_path)

        gemini_logger.info(
            "‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ –ë–î: %d/%d",
            successful_sends,
            len([r for r in results if r.get("status") == "success"]),
        )
        return True

    except Exception as fallback_error:
        gemini_logger.error("‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: %s", fallback_error)
        return False


def extract_tender_id(json_path: Path, tender_data: Dict) -> str:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç ID —Ç–µ–Ω–¥–µ—Ä–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

    Args:
        json_path: –ü—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
        tender_data: –î–∞–Ω–Ω—ã–µ —Ç–µ–Ω–¥–µ—Ä–∞

    Returns:
        ID —Ç–µ–Ω–¥–µ—Ä–∞
    """
    # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å ID –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    tender_id = tender_data.get("tender_id") or tender_data.get("db_id")

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
    if not tender_id:
        tender_id = json_path.stem

    return str(tender_id)


def get_processing_status(tender_id: str, lot_ids: List[str], redis_config: Optional[Dict] = None) -> Dict:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å AI –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è —Ç–µ–Ω–¥–µ—Ä–∞ (—á–µ—Ä–µ–∑ Redis-–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é Gemini).
    """
    try:
        integration = GeminiIntegration.from_redis_config(redis_config or {})
    except Exception:
        return {"error": "Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"}

    return integration.get_processing_status(tender_id, lot_ids)


def _import_full_tender_via_go(processed_data: dict) -> tuple[str, dict[str, int]]:
    """
    –®–ª—ë—Ç json_1 –≤ Go `/api/v1/import-tender`, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (db_id, lot_ids_map).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Idempotency-Key –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö.
    –ë—Ä–æ—Å–∞–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """
    go_url = os.getenv("GO_SERVER_API_ENDPOINT")
    api_key = os.getenv("GO_SERVER_API_KEY")
    if not go_url:
        raise RuntimeError("GO_SERVER_API_ENDPOINT –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

    base = go_url.rstrip("/")
    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ –ø–æ–ª–Ω–æ–≥–æ –ø—É—Ç–∏, —Ç–∞–∫ –∏ –±–∞–∑–æ–≤–æ–≥–æ /api/v1
    if base.endswith("/import-tender"):
        url = base
    else:
        url = f"{base}/import-tender"

    headers = {}
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"

    # –î–æ–±–∞–≤–ª—è–µ–º Idempotency-Key –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏ retry
    try:
        import hashlib
        import json as _json
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Ö–µ—à –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ –¥–∞–Ω–Ω—ã—Ö
        idem_key = hashlib.sha256(
            _json.dumps(processed_data, ensure_ascii=False, sort_keys=True).encode("utf-8")
        ).hexdigest()
        headers["Idempotency-Key"] = idem_key
        log.debug(f"Idempotency-Key: {idem_key[:16]}...")
    except Exception:
        log.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã—á–∏—Å–ª–∏—Ç—å Idempotency-Key; –ø—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –Ω–µ–≥–æ", exc_info=True)

    timeout = float(os.getenv("GO_HTTP_TIMEOUT", "60"))

    try:
        resp = requests.post(
            url,
            json=processed_data,
            headers=headers,
            timeout=(5, timeout),
        )
    except requests.RequestException as e:
        raise RuntimeError(f"Go import network error: {e}") from e
    if resp.status_code >= 400:
        raise RuntimeError(f"Go import failed: {resp.status_code} {resp.text}")

    try:
        data = resp.json()
    except ValueError:
        raise RuntimeError(f"Go import: –Ω–µ-JSON –æ—Ç–≤–µ—Ç: {resp.text[:500]}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ db_id
    db_id_val = data.get("db_id")
    if not db_id_val:
        raise RuntimeError("Go import: empty db_id")
    db_id = str(db_id_val)

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ lot_ids
    raw_lots = data.get("lots_id") or {}
    lots_map = {}
    for k, v in raw_lots.items():
        try:
            lots_map[str(k)] = int(v)
        except (TypeError, ValueError):
            log.warning("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π lot_id –¥–ª—è %r: %r", k, v)

    return db_id, lots_map


def main():
    """–ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å Gemini"""
    parser = argparse.ArgumentParser(description="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–Ω–¥–µ—Ä–æ–≤ —Å Gemini AI")

    subparsers = parser.add_subparsers(dest="command", help="–ö–æ–º–∞–Ω–¥—ã")

    # –ö–æ–º–∞–Ω–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
    process_parser = subparsers.add_parser("process", help="–û–±—Ä–∞–±–æ—Ç–∞—Ç—å XLSX —Ñ–∞–π–ª")
    process_parser.add_argument("xlsx_file", help="–ü—É—Ç—å –∫ XLSX —Ñ–∞–π–ª—É")
    process_parser.add_argument("--ai", action="store_true", help="–í–∫–ª—é—á–∏—Ç—å AI –æ–±—Ä–∞–±–æ—Ç–∫—É")
    # Back-compat + –Ω–æ–≤—ã–π —Ñ–ª–∞–≥
    process_parser.add_argument(
        "--async", dest="async_mode", action="store_true", help="–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (DEPRECATED, use --async-mode)"
    )
    process_parser.add_argument("--async-mode", dest="async_mode", action="store_true", help="–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
    process_parser.add_argument("--redis-host", default="localhost", help="–•–æ—Å—Ç Redis")
    process_parser.add_argument("--redis-port", type=int, default=6379, help="–ü–æ—Ä—Ç Redis")
    process_parser.add_argument("--redis-db", type=int, default=0, help="–ë–∞–∑–∞ Redis")

    # –ö–æ–º–∞–Ω–¥–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞
    status_parser = subparsers.add_parser("status", help="–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç—É—Å –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    status_parser.add_argument("tender_id", help="ID —Ç–µ–Ω–¥–µ—Ä–∞")
    status_parser.add_argument("lot_ids", nargs="+", help="ID –ª–æ—Ç–æ–≤")
    status_parser.add_argument("--redis-host", default="localhost", help="–•–æ—Å—Ç Redis")
    status_parser.add_argument("--redis-port", type=int, default=6379, help="–ü–æ—Ä—Ç Redis")
    status_parser.add_argument("--redis-db", type=int, default=0, help="–ë–∞–∑–∞ Redis")

    parser.add_argument("--verbose", "-v", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –≤—ã–≤–æ–¥")

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        return 1

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–ª—å–∫–æ –≤ CLI-—Ä–µ–∂–∏–º–µ (–Ω–µ –ª–æ–º–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ —Å–µ—Ä–≤–∏—Å–∞/–≤–æ—Ä–∫–µ—Ä–∞)
    log_level = os.getenv("LOG_LEVEL", "INFO").upper() if not args.verbose else "DEBUG"
    gemini_log_level = os.getenv("GEMINI_LOG_LEVEL", "INFO").upper() if not args.verbose else "DEBUG"

    logging.getLogger().setLevel(getattr(logging, log_level, logging.INFO))
    logging.getLogger("app").setLevel(getattr(logging, log_level, logging.INFO))
    get_gemini_logger().setLevel(getattr(logging, gemini_log_level, logging.INFO))

    # –§–æ—Ä–º–∞—Ç –ª–æ–≥–æ–≤
    log_format = (
        "%(levelname)s:%(name)s:%(funcName)s:%(lineno)d:%(message)s"
        if args.verbose
        else "%(levelname)s:%(name)s:%(message)s"
    )

    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ handlers –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π
    root_logger = logging.getLogger()
    if not root_logger.handlers:
        handler = logging.StreamHandler()
        handler.setLevel(getattr(logging, log_level, logging.INFO))
        formatter = logging.Formatter(log_format)
        handler.setFormatter(formatter)
        root_logger.addHandler(handler)
    else:
        for handler in root_logger.handlers:
            handler.setLevel(getattr(logging, log_level, logging.INFO))
            formatter = logging.Formatter(log_format)
            handler.setFormatter(formatter)

    redis_config = {
        "host": getattr(args, "redis_host", "localhost"),
        "port": getattr(args, "redis_port", 6379),
        "db": getattr(args, "redis_db", 0),
    }

    try:
        if args.command == "process":
            success = parse_file_with_gemini(
                xlsx_path=args.xlsx_file,
                enable_ai=getattr(args, "ai", False),
                async_processing=getattr(args, "async_mode", False),
                redis_config=redis_config,
            )
            return 0 if success else 1

        elif args.command == "status":
            statuses = get_processing_status(args.tender_id, args.lot_ids, redis_config)
            print(json.dumps(statuses, ensure_ascii=False, indent=2))
            return 0

    except KeyboardInterrupt:
        print("\nüõë –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return 1
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())
