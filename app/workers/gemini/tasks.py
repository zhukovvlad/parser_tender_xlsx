# app/workers/gemini/tasks.py

"""
Celery –∑–∞–¥–∞—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–Ω–¥–µ—Ä–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π —Å –ø–æ–º–æ—â—å—é Gemini AI.
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º GeminiWorker.
"""

import shutil
from pathlib import Path
from typing import Any, Dict

from celery.utils.log import get_task_logger

from ...celery_app import celery_app
from ...gemini_module.constants import (
    FALLBACK_CATEGORY,
    TENDER_CATEGORIES,
    TENDER_CONFIGS,
)
from ...go_module import update_lot_ai_results_sync
from ...json_to_server.ai_results_client import save_ai_results_offline
from .worker import GeminiWorker

# –õ–æ–≥–≥–µ—Ä –¥–ª—è Celery –∑–∞–¥–∞—á
logger = get_task_logger(__name__)


@celery_app.task(
    bind=True,
    queue="ai_queue",  # –ù–∞–ø—Ä–∞–≤–ª—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –æ—á–µ—Ä–µ–¥—å –¥–ª—è AI –∑–∞–¥–∞—á
    autoretry_for=(Exception,),
    retry_kwargs={"max_retries": 5, "countdown": 60},
    rate_limit="10/m",  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –Ω–µ –±–æ–ª–µ–µ 10 –∑–∞–¥–∞—á –≤ –º–∏–Ω—É—Ç—É –Ω–∞ –≤–æ—Ä–∫–µ—Ä
)
def process_tender_positions(
    self, tender_id: str, lot_id: str, positions_file_path: str, api_key: str
) -> Dict[str, Any]:
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ –ø–æ–∑–∏—Ü–∏–π –ª–æ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Gemini AI.

    Args:
        self: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ Celery (bind=True)
        tender_id: ID —Ç–µ–Ω–¥–µ—Ä–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        lot_id: ID –ª–æ—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        positions_file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É _positions.md
        api_key: Google API –∫–ª—é—á –¥–ª—è Gemini

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏

    Raises:
        Exception: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º retry)
    """
    task_id = self.request.id
    logger.info(f"üöÄ Starting Gemini AI processing for tender {tender_id}, lot {lot_id} (task: {task_id})")

    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
        self.update_state(
            state="PROCESSING", meta={"tender_id": tender_id, "lot_id": lot_id, "stage": "initializing", "progress": 0}
        )

        # API –∫–ª—é—á –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä
        if not api_key:
            raise ValueError("API key is required but not provided")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        positions_file = Path(positions_file_path)
        if not positions_file.exists():
            raise FileNotFoundError(f"Positions file not found: {positions_file_path}")

        logger.info(f"üìÅ Processing file: {positions_file_path}")

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        self.update_state(
            state="PROCESSING",
            meta={"tender_id": tender_id, "lot_id": lot_id, "stage": "ai_processing", "progress": 25},
        )

        # –°–æ–∑–¥–∞–µ–º –≤–æ—Ä–∫–µ—Ä –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        worker = GeminiWorker(api_key)

        result = worker.process_positions_file(
            tender_id=tender_id,
            lot_id=lot_id,
            positions_file_path=positions_file_path,
            categories=TENDER_CATEGORIES,
            configs=TENDER_CONFIGS,
            fallback_category=FALLBACK_CATEGORY,
        )

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        self.update_state(
            state="PROCESSING", meta={"tender_id": tender_id, "lot_id": lot_id, "stage": "finalizing", "progress": 90}
        )

        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if result.get("status") == "success":
            logger.info(f"‚úÖ Successfully processed {tender_id}_{lot_id}. Category: {result.get('category')}")
            logger.info(f"üìä Extracted {len(result.get('ai_data', {}))} fields")
        else:
            logger.warning(f"‚ö†Ô∏è Processing completed with issues: {result.get('error')}")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –ë–î —á–µ—Ä–µ–∑ Go-—Å–µ—Ä–≤–∏—Å (–∏ –æ—Ñ—Ñ–ª–∞–π–Ω-—Ñ–æ–ª–±—ç–∫)
        if result.get("status") == "success":
            try:
                update_lot_ai_results_sync(
                    lot_db_id=str(lot_id),
                    tender_id=str(tender_id),  # –ü–µ—Ä–µ–¥–∞–µ–º tender_id
                    category=result.get("category", ""),
                    ai_data=result.get("ai_data", {}),
                    processed_at=result.get("processed_at", ""),
                )
                logger.info(f"üíæ AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –Ω–∞ Go –¥–ª—è {tender_id}_{lot_id}")

                # –†–µ–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã —Å AI –¥–∞–Ω–Ω—ã–º–∏
                try:
                    from app.markdown_utils.regeneration_utils import regenerate_reports_for_lot

                    regenerate_reports_for_lot(
                        tender_id=tender_id,
                        lot_id=lot_id,
                        ai_result=result,
                        logger=logger,
                    )
                except Exception:
                    logger.exception(
                        "‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ä–µ–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–æ–≤ –¥–ª—è –ª–æ—Ç–∞ %s_%s",
                        tender_id,
                        lot_id,
                    )
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ Go –¥–ª—è {tender_id}_{lot_id}: {e}")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ñ—Ñ–ª–∞–π–Ω –ø—Ä–∏ –æ—à–∏–±–∫–µ
                offline_path = save_ai_results_offline(
                    tender_id=tender_id,
                    lot_id=lot_id,
                    category=result.get("category", ""),
                    ai_data=result.get("ai_data", {}),
                    processed_at=result.get("processed_at", ""),
                    reason="request_failed",
                )
                logger.warning(f"üì¶ AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –æ—Ñ—Ñ–ª–∞–π–Ω: {offline_path}")

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
        self.update_state(
            state="SUCCESS",
            meta={"tender_id": tender_id, "lot_id": lot_id, "stage": "completed", "progress": 100, "result": result},
        )

        # –ê—Ä—Ö–∏–≤–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ñ–∞–π–ª (–ø–µ—Ä–µ–º–µ—â–∞–µ–º –≤ finalized –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é)
        try:
            _archive_processed_file(positions_file_path)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not archive file {positions_file_path}: {e}")

        return result

    except Exception as e:
        logger.error(f"‚ùå Error processing {tender_id}_{lot_id}: {str(e)}")

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—à–∏–±–∫–∏
        self.update_state(
            state="FAILURE",
            meta={"tender_id": tender_id, "lot_id": lot_id, "stage": "failed", "error": str(e), "progress": 0},
        )
        raise


def _archive_processed_file(file_path: str):
    """
    –ê—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –∏–∑ pending_sync_positions –≤ tenders_positions.

    Args:
        file_path: –ü—É—Ç—å –∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É
    """
    import shutil
    from pathlib import Path

    source_path = Path(file_path)
    if not source_path.exists():
        return

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
    if "pending_sync_positions" in str(source_path):
        target_dir = Path("tenders_positions")
        target_dir.mkdir(exist_ok=True)

        target_path = target_dir / source_path.name
        shutil.move(str(source_path), str(target_path))

        logger.info(f"üìÇ File archived: {source_path.name} -> tenders_positions/")


@celery_app.task(bind=True)
def process_tender_batch(self, tender_id: str, lots_data: list, api_key: str) -> Dict[str, Any]:
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –ª–æ—Ç–æ–≤ —Ç–µ–Ω–¥–µ—Ä–∞.

    Args:
        self: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ Celery
        tender_id: ID —Ç–µ–Ω–¥–µ—Ä–∞
        lots_data: –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –ª–æ—Ç–æ–≤ —Å positions_file_path
        api_key: Google API –∫–ª—é—á –¥–ª—è Gemini

    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –ª–æ—Ç–æ–≤
    """
    task_id = self.request.id
    logger.info(f"üîÑ Starting batch processing for tender {tender_id} with {len(lots_data)} lots (task: {task_id})")

    try:
        self.update_state(
            state="PROCESSING",
            meta={
                "tender_id": tender_id,
                "stage": "batch_processing",
                "total_lots": len(lots_data),
                "processed_lots": 0,
                "progress": 0,
            },
        )

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –ø–æ–¥–∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
        subtask_ids = []
        for i, lot_data in enumerate(lots_data):
            lot_id = lot_data.get("lot_id")
            positions_file = lot_data.get("positions_file_path")

            logger.info(f"üìù Starting async processing for lot {i+1}/{len(lots_data)}: {lot_id}")

            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–¥–∑–∞–¥–∞—á—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π (Rate Limiting)
            # –°–¥–≤–∏–≥–∞–µ–º –∑–∞–ø—É—Å–∫ –∫–∞–∂–¥–æ–π —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–∏ –Ω–∞ 4 —Å–µ–∫—É–Ω–¥—ã, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å RPM –ª–∏–º–∏—Ç
            delay_seconds = i * 4
            subtask = process_tender_positions.apply_async(
                args=[tender_id, lot_id, positions_file, api_key], countdown=delay_seconds
            )
            subtask_ids.append(subtask.id)

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å - –≤—Å–µ –∑–∞–¥–∞—á–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã
        self.update_state(
            state="PROCESSING",
            meta={
                "tender_id": tender_id,
                "stage": "tasks_dispatched",
                "total_lots": len(lots_data),
                "dispatched_tasks": len(subtask_ids),
                "subtask_ids": subtask_ids,
                "progress": 50,
            },
        )

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç batch –æ–ø–µ—Ä–∞—Ü–∏–∏
        batch_result = {
            "tender_id": tender_id,
            "total_lots": len(lots_data),
            "dispatched_tasks": len(subtask_ids),
            "subtask_ids": subtask_ids,
            "status": "dispatched",
            "message": f"–ó–∞–ø—É—â–µ–Ω–æ {len(subtask_ids)} –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–æ—Ç–æ–≤",
        }

        self.update_state(
            state="SUCCESS",
            meta={"tender_id": tender_id, "stage": "completed", "progress": 100, "batch_result": batch_result},
        )

        logger.info(f"‚úÖ Batch processing dispatched for {tender_id}: {len(subtask_ids)} tasks started")
        return batch_result

    except Exception as e:
        logger.error(f"‚ùå Batch processing error for {tender_id}: {str(e)}")

        self.update_state(
            state="FAILURE", meta={"tender_id": tender_id, "stage": "failed", "error": str(e), "progress": 0}
        )
        raise


@celery_app.task
def cleanup_old_results():
    """
    –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
    –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é.
    """
    logger.info("üßπ Starting cleanup of old results and temporary files")

    try:
        cleanup_stats = {"temp_uploads": 0, "pending_sync_positions": 0, "redis_keys": 0}

        import time
        from pathlib import Path

        current_time = time.time()

        # 1. –û—á–∏—Å—Ç–∫–∞ temp_uploads (—Ñ–∞–π–ª—ã —Å—Ç–∞—Ä—à–µ 24 —á–∞—Å–æ–≤)
        temp_dir = Path("temp_uploads")
        if temp_dir.exists():
            for file_path in temp_dir.iterdir():
                if file_path.is_file():
                    file_age = current_time - file_path.stat().st_mtime
                    if file_age > 86400:  # 24 —á–∞—Å–∞
                        file_path.unlink()
                        cleanup_stats["temp_uploads"] += 1

        # 2. –ê—Ä—Ö–∏–≤–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö positions —Ñ–∞–π–ª–æ–≤ (—Å—Ç–∞—Ä—à–µ 6 —á–∞—Å–æ–≤)
        pending_positions_dir = Path("pending_sync_positions")
        tenders_positions_dir = Path("tenders_positions")
        tenders_positions_dir.mkdir(exist_ok=True)

        if pending_positions_dir.exists():
            for file_path in pending_positions_dir.iterdir():
                if file_path.is_file() and file_path.suffix == ".md":
                    file_age = current_time - file_path.stat().st_mtime
                    if file_age > 21600:  # 6 —á–∞—Å–æ–≤
                        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ —Ñ–∏–Ω–∞–ª—å–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                        target_path = tenders_positions_dir / file_path.name
                        shutil.move(str(file_path), str(target_path))
                        cleanup_stats["pending_sync_positions"] += 1
                        logger.info(f"üìÇ Archived: {file_path.name} -> tenders_positions/")

        # 3. –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö Redis –∫–ª—é—á–µ–π (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—á–∏—Å—Ç–∫—É –∫–ª—é—á–µ–π –∑–∞–¥–∞—á —Å—Ç–∞—Ä—à–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏

        logger.info(f"‚úÖ Cleanup completed: {cleanup_stats}")
        return cleanup_stats

    except Exception as e:
        logger.error(f"‚ùå Cleanup error: {str(e)}")
        raise
