# app/workers/gemini/tasks.py

"""
Celery –∑–∞–¥–∞—á–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–Ω–¥–µ—Ä–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π —Å –ø–æ–º–æ—â—å—é Gemini AI.
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º GeminiWorker.
"""

import json
import os
from pathlib import Path
from typing import Dict, Any

from celery import current_task
from celery.utils.log import get_task_logger

from ...celery_app import celery_app
from ...gemini_module.constants import TENDER_CATEGORIES, TENDER_CONFIGS, FALLBACK_CATEGORY
from .worker import GeminiWorker

# –õ–æ–≥–≥–µ—Ä –¥–ª—è Celery –∑–∞–¥–∞—á
logger = get_task_logger(__name__)


@celery_app.task(bind=True, autoretry_for=(Exception,), retry_kwargs={'max_retries': 3, 'countdown': 60})
def process_tender_positions(self, tender_id: str, lot_id: str, positions_file_path: str, api_key: str) -> Dict[str, Any]:
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ –ø–æ–∑–∏—Ü–∏–π –ª–æ—Ç–∞ —Å –ø–æ–º–æ—â—å—é Gemini AI.
    
    Args:
        self: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ Celery (bind=True)
        tender_id: ID —Ç–µ–Ω–¥–µ—Ä–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        lot_id: ID –ª–æ—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö  
        positions_file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É _positions.md
        api_key: Google API –∫–ª—é—á –¥–ª—è Gemini
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
    Raises:
        Exception: –ü—Ä–∏ –æ—à–∏–±–∫–∞—Ö –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º retry)
    """
    task_id = self.request.id
    logger.info(f"üöÄ Starting Gemini AI processing for tender {tender_id}, lot {lot_id} (task: {task_id})")
    
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏
        self.update_state(
            state='PROCESSING',
            meta={
                'tender_id': tender_id,
                'lot_id': lot_id,
                'stage': 'initializing',
                'progress': 0
            }
        )
        
        # API –∫–ª—é—á –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –∫–∞–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä
        if not api_key:
            raise ValueError("API key is required but not provided")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        positions_file = Path(positions_file_path)
        if not positions_file.exists():
            raise FileNotFoundError(f"Positions file not found: {positions_file_path}")
        
        logger.info(f"üìÅ Processing file: {positions_file_path}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        self.update_state(
            state='PROCESSING',
            meta={
                'tender_id': tender_id,
                'lot_id': lot_id,
                'stage': 'ai_processing',
                'progress': 25
            }
        )
        
        # –°–æ–∑–¥–∞–µ–º –≤–æ—Ä–∫–µ—Ä –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
        worker = GeminiWorker(api_key)
        
        result = worker.process_positions_file(
            tender_id=tender_id,
            lot_id=lot_id,
            positions_file_path=positions_file_path,
            categories=TENDER_CATEGORIES,
            configs=TENDER_CONFIGS,
            fallback_category=FALLBACK_CATEGORY
        )
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        self.update_state(
            state='PROCESSING',
            meta={
                'tender_id': tender_id,
                'lot_id': lot_id,
                'stage': 'finalizing',
                'progress': 90
            }
        )
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if result.get("status") == "success":
            logger.info(f"‚úÖ Successfully processed {tender_id}_{lot_id}. Category: {result.get('category')}")
            logger.info(f"üìä Extracted {len(result.get('ai_data', {}))} fields")
        else:
            logger.warning(f"‚ö†Ô∏è Processing completed with issues: {result.get('error')}")
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞
        self.update_state(
            state='SUCCESS',
            meta={
                'tender_id': tender_id,
                'lot_id': lot_id,
                'stage': 'completed',
                'progress': 100,
                'result': result
            }
        )
        
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error processing {tender_id}_{lot_id}: {str(e)}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –æ—à–∏–±–∫–∏
        self.update_state(
            state='FAILURE',
            meta={
                'tender_id': tender_id,
                'lot_id': lot_id,
                'stage': 'failed',
                'error': str(e),
                'progress': 0
            }
        )
        raise


@celery_app.task(bind=True)
def process_tender_batch(self, tender_id: str, lots_data: list, api_key: str) -> Dict[str, Any]:
    """
    Celery –∑–∞–¥–∞—á–∞ –¥–ª—è batch –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –ª–æ—Ç–æ–≤ —Ç–µ–Ω–¥–µ—Ä–∞.
    
    Args:
        self: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∑–∞–¥–∞—á–∏ Celery
        tender_id: ID —Ç–µ–Ω–¥–µ—Ä–∞
        lots_data: –°–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö –ª–æ—Ç–æ–≤ —Å positions_file_path
        api_key: Google API –∫–ª—é—á –¥–ª—è Gemini
        
    Returns:
        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö –ª–æ—Ç–æ–≤
    """
    task_id = self.request.id
    logger.info(f"üîÑ Starting batch processing for tender {tender_id} with {len(lots_data)} lots (task: {task_id})")
    
    try:
        self.update_state(
            state='PROCESSING',
            meta={
                'tender_id': tender_id,
                'stage': 'batch_processing',
                'total_lots': len(lots_data),
                'processed_lots': 0,
                'progress': 0
            }
        )
        
        results = []
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –ø–æ–¥–∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ (–∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)
        subtask_ids = []
        for i, lot_data in enumerate(lots_data):
            lot_id = lot_data.get("lot_id")
            positions_file = lot_data.get("positions_file_path")
            
            logger.info(f"üìù Starting async processing for lot {i+1}/{len(lots_data)}: {lot_id}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–æ–¥–∑–∞–¥–∞—á—É –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ (–±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è)
            subtask = process_tender_positions.delay(tender_id, lot_id, positions_file, api_key)
            subtask_ids.append(subtask.id)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å - –≤—Å–µ –∑–∞–¥–∞—á–∏ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã
        self.update_state(
            state='PROCESSING',
            meta={
                'tender_id': tender_id,
                'stage': 'tasks_dispatched',
                'total_lots': len(lots_data),
                'dispatched_tasks': len(subtask_ids),
                'subtask_ids': subtask_ids,
                'progress': 50
            }
        )
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç batch –æ–ø–µ—Ä–∞—Ü–∏–∏
        batch_result = {
            'tender_id': tender_id,
            'total_lots': len(lots_data),
            'dispatched_tasks': len(subtask_ids),
            'subtask_ids': subtask_ids,
            'status': 'dispatched',
            'message': f'–ó–∞–ø—É—â–µ–Ω–æ {len(subtask_ids)} –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ª–æ—Ç–æ–≤'
        }
        
        self.update_state(
            state='SUCCESS',
            meta={
                'tender_id': tender_id,
                'stage': 'completed',
                'progress': 100,
                'batch_result': batch_result
            }
        )
        
        logger.info(f"‚úÖ Batch processing dispatched for {tender_id}: {len(subtask_ids)} tasks started")
        return batch_result
        
    except Exception as e:
        logger.error(f"‚ùå Batch processing error for {tender_id}: {str(e)}")
        
        self.update_state(
            state='FAILURE',
            meta={
                'tender_id': tender_id,
                'stage': 'failed',
                'error': str(e),
                'progress': 0
            }
        )
        raise


@celery_app.task
def cleanup_old_results():
    """
    –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.
    –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –ø–æ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—é.
    """
    logger.info("üßπ Starting cleanup of old results and temporary files")
    
    try:
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –æ—á–∏—Å—Ç–∫–∏:
        # - –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏–∑ temp_uploads
        # - –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ Redis
        # - –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ –ª–æ–≥–æ–≤
        
        cleanup_count = 0
        
        # –ü—Ä–∏–º–µ—Ä: –æ—á–∏—Å—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ —Å—Ç–∞—Ä—à–µ 24 —á–∞—Å–æ–≤
        import time
        from pathlib import Path
        
        temp_dir = Path("temp_uploads")
        if temp_dir.exists():
            current_time = time.time()
            for file_path in temp_dir.iterdir():
                if file_path.is_file():
                    file_age = current_time - file_path.stat().st_mtime
                    if file_age > 86400:  # 24 —á–∞—Å–∞
                        file_path.unlink()
                        cleanup_count += 1
        
        logger.info(f"‚úÖ Cleanup completed. Removed {cleanup_count} old files")
        return {"cleaned_files": cleanup_count}
        
    except Exception as e:
        logger.error(f"‚ùå Cleanup error: {str(e)}")
        raise


# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á
from celery.schedules import crontab

celery_app.conf.beat_schedule = {
    'cleanup-old-results': {
        'task': 'app.workers.gemini.tasks.cleanup_old_results',
        'schedule': crontab(hour=2, minute=0),  # –ö–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 2:00
    },
}
celery_app.conf.timezone = 'UTC'
